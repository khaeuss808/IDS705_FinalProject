{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load and preprocess your English-labeled data\n",
    "df = pd.read_csv(\"1900rows_data.csv\")[[\"text\", \"LABEL\"]]\n",
    "df = df.rename(columns={\"text\": \"phrase\", \"LABEL\": \"label\"})\n",
    "\n",
    "# Encode labels\n",
    "unique_labels = df[\"label\"].unique().tolist()\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "df[\"label_id\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_ds = Dataset.from_pandas(train_df[[\"phrase\", \"label_id\"]])\n",
    "test_ds = Dataset.from_pandas(test_df[[\"phrase\", \"label_id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b64a98819284806971e535067a0e293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1524 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552630704dc541128f812d9789e431d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/382 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"phrase\"], padding=\"max_length\", truncation=True, max_length=128\n",
    "    )\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(tokenize_function, batched=True)\n",
    "test_ds = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "train_ds = train_ds.rename_column(\"label_id\", \"labels\")\n",
    "test_ds = test_ds.rename_column(\"label_id\", \"labels\")\n",
    "train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/gj/nvvqk8_x3l56pvm3zf2l8g_r0000gn/T/ipykernel_51847/122630788.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\", num_labels=len(label2id)\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./multilingual_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa49508204f4c6cb2c82dc10bf8c66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/573 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1397, 'grad_norm': 12.840332984924316, 'learning_rate': 4.56369982547993e-05, 'epoch': 0.26}\n",
      "{'loss': 1.7763, 'grad_norm': 15.957270622253418, 'learning_rate': 4.12739965095986e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2525, 'grad_norm': 7.2755842208862305, 'learning_rate': 3.691099476439791e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e09cce88bd407182895e5dd709fed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9233168363571167, 'eval_accuracy': 0.7146596858638743, 'eval_f1': 0.6857415134116677, 'eval_runtime': 4.1268, 'eval_samples_per_second': 92.565, 'eval_steps_per_second': 11.631, 'epoch': 1.0}\n",
      "{'loss': 0.9348, 'grad_norm': 11.172807693481445, 'learning_rate': 3.254799301919721e-05, 'epoch': 1.05}\n",
      "{'loss': 0.6055, 'grad_norm': 2.9885454177856445, 'learning_rate': 2.8184991273996508e-05, 'epoch': 1.31}\n",
      "{'loss': 0.5925, 'grad_norm': 6.894710540771484, 'learning_rate': 2.382198952879581e-05, 'epoch': 1.57}\n",
      "{'loss': 0.5689, 'grad_norm': 22.988672256469727, 'learning_rate': 1.9458987783595115e-05, 'epoch': 1.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c9695d050041678e0b6121437184c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5195261240005493, 'eval_accuracy': 0.8481675392670157, 'eval_f1': 0.8429289395713538, 'eval_runtime': 3.6841, 'eval_samples_per_second': 103.689, 'eval_steps_per_second': 13.029, 'epoch': 2.0}\n",
      "{'loss': 0.4215, 'grad_norm': 21.366735458374023, 'learning_rate': 1.5095986038394417e-05, 'epoch': 2.09}\n",
      "{'loss': 0.2199, 'grad_norm': 11.411602020263672, 'learning_rate': 1.0732984293193717e-05, 'epoch': 2.36}\n",
      "{'loss': 0.2294, 'grad_norm': 1.2310848236083984, 'learning_rate': 6.369982547993019e-06, 'epoch': 2.62}\n",
      "{'loss': 0.2625, 'grad_norm': 19.81229591369629, 'learning_rate': 2.006980802792321e-06, 'epoch': 2.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a5e6edb4794cc78d6932b2b294aaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.40276291966438293, 'eval_accuracy': 0.8717277486910995, 'eval_f1': 0.8694356763187971, 'eval_runtime': 3.7316, 'eval_samples_per_second': 102.368, 'eval_steps_per_second': 12.863, 'epoch': 3.0}\n",
      "{'train_runtime': 228.1192, 'train_samples_per_second': 20.042, 'train_steps_per_second': 2.512, 'train_loss': 0.7933010181207307, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"multilingual_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Zero-Shot Inference on Chinese Text with Your Multilingual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_path = \"multilingual_output\"  # your saved model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "import pandas as pd\n",
    "\n",
    "# Load and sample your English data\n",
    "df = pd.read_csv(\"1900rows_data.csv\")[[\"text\", \"LABEL\"]].rename(\n",
    "    columns={\"text\": \"phrase\", \"LABEL\": \"label\"}\n",
    ")\n",
    "sample_df = df.sample(10, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Translate English to Chinese\n",
    "sample_df[\"translated_phrase\"] = sample_df[\"phrase\"].apply(\n",
    "    lambda x: GoogleTranslator(source=\"en\", target=\"zh-CN\").translate(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and predict\n",
    "inputs = tokenizer(\n",
    "    sample_df[\"translated_phrase\"].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>translated_phrase</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've been feeling really weak in my muscles an...</td>\n",
       "      <td>我的肌肉感觉真的很虚弱，脖子真的很僵硬。我的关节一直在肿胀，我很难四处走动而不会感到僵硬。步...</td>\n",
       "      <td>Chronic Conditions</td>\n",
       "      <td>Chronic Conditions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when i extend my leg there is pain in knee joint</td>\n",
       "      <td>当我伸出腿时，膝关节会疼痛</td>\n",
       "      <td>Pain-related Conditions</td>\n",
       "      <td>Pain-related Conditions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My bloody stools have caused me to lose a lot ...</td>\n",
       "      <td>我的血腥凳子使我失去了很多东西，包括铁和蓝色。结果，我现在患有贫血，通常我会感到很虚弱。</td>\n",
       "      <td>Gastrointestinal Conditions</td>\n",
       "      <td>Infections</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My skin is red and scratchy. These can occasio...</td>\n",
       "      <td>我的皮肤是红色的。这些偶尔会剥落。我的脸颊和嘴唇肿胀，这真的很烦人。我偶尔会头痛和流鼻涕，因...</td>\n",
       "      <td>Allergic/Immunologic Reactions</td>\n",
       "      <td>Allergic/Immunologic Reactions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have a cut on my foot that became infected f...</td>\n",
       "      <td>我的脚割伤了，由于在健身房使用淋浴而被感染。</td>\n",
       "      <td>Infections</td>\n",
       "      <td>Infections</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I have been experiencing symptoms such as a he...</td>\n",
       "      <td>我一直在遇到症状，例如头痛，胸痛，头晕，平衡丧失和困难。</td>\n",
       "      <td>Chronic Conditions</td>\n",
       "      <td>Chronic Conditions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>My knee hurts when I walk</td>\n",
       "      <td>我走路时膝盖疼</td>\n",
       "      <td>Pain-related Conditions</td>\n",
       "      <td>Pain-related Conditions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>It feels like I can't take a deep breath</td>\n",
       "      <td>感觉我不能深吸一口气</td>\n",
       "      <td>Respiratory &amp; Sensory Issues</td>\n",
       "      <td>Respiratory &amp; Sensory Issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I have a cut that is red and swollen.</td>\n",
       "      <td>我有一个红色和肿胀的切口。</td>\n",
       "      <td>Infections</td>\n",
       "      <td>Dermatological &amp; Skin Conditions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I have a high temperature, vomiting, chills, a...</td>\n",
       "      <td>我有高温，呕吐，发冷和严重的瘙痒。此外，我一直在说话很多，头痛。我也因恶心和肌肉疼痛而困扰。</td>\n",
       "      <td>Infections</td>\n",
       "      <td>Allergic/Immunologic Reactions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              phrase  \\\n",
       "0  I've been feeling really weak in my muscles an...   \n",
       "1   when i extend my leg there is pain in knee joint   \n",
       "2  My bloody stools have caused me to lose a lot ...   \n",
       "3  My skin is red and scratchy. These can occasio...   \n",
       "4  I have a cut on my foot that became infected f...   \n",
       "5  I have been experiencing symptoms such as a he...   \n",
       "6                          My knee hurts when I walk   \n",
       "7           It feels like I can't take a deep breath   \n",
       "8              I have a cut that is red and swollen.   \n",
       "9  I have a high temperature, vomiting, chills, a...   \n",
       "\n",
       "                                   translated_phrase  \\\n",
       "0  我的肌肉感觉真的很虚弱，脖子真的很僵硬。我的关节一直在肿胀，我很难四处走动而不会感到僵硬。步...   \n",
       "1                                      当我伸出腿时，膝关节会疼痛   \n",
       "2       我的血腥凳子使我失去了很多东西，包括铁和蓝色。结果，我现在患有贫血，通常我会感到很虚弱。   \n",
       "3  我的皮肤是红色的。这些偶尔会剥落。我的脸颊和嘴唇肿胀，这真的很烦人。我偶尔会头痛和流鼻涕，因...   \n",
       "4                             我的脚割伤了，由于在健身房使用淋浴而被感染。   \n",
       "5                       我一直在遇到症状，例如头痛，胸痛，头晕，平衡丧失和困难。   \n",
       "6                                            我走路时膝盖疼   \n",
       "7                                         感觉我不能深吸一口气   \n",
       "8                                      我有一个红色和肿胀的切口。   \n",
       "9     我有高温，呕吐，发冷和严重的瘙痒。此外，我一直在说话很多，头痛。我也因恶心和肌肉疼痛而困扰。   \n",
       "\n",
       "                            label                   predicted_label  \n",
       "0              Chronic Conditions                Chronic Conditions  \n",
       "1         Pain-related Conditions           Pain-related Conditions  \n",
       "2     Gastrointestinal Conditions                        Infections  \n",
       "3  Allergic/Immunologic Reactions    Allergic/Immunologic Reactions  \n",
       "4                      Infections                        Infections  \n",
       "5              Chronic Conditions                Chronic Conditions  \n",
       "6         Pain-related Conditions           Pain-related Conditions  \n",
       "7    Respiratory & Sensory Issues      Respiratory & Sensory Issues  \n",
       "8                      Infections  Dermatological & Skin Conditions  \n",
       "9                      Infections    Allergic/Immunologic Reactions  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label mapping (rebuild from training)\n",
    "unique_labels = df[\"label\"].unique().tolist()\n",
    "id2label = {i: label for i, label in enumerate(unique_labels)}\n",
    "\n",
    "# Add predictions to DataFrame\n",
    "sample_df[\"predicted_label\"] = [id2label[p.item()] for p in predictions]\n",
    "\n",
    "# Show results\n",
    "sample_df[[\"phrase\", \"translated_phrase\", \"label\", \"predicted_label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We fine-tuned bert-base-multilingual-cased on 1,900 English-labeled medical symptom examples. We then evaluated the model on Chinese-translated versions of those examples, without any fine-tuning on Chinese data. The model correctly classified most inputs, achieving strong semantic generalization across languages. These results demonstrate the power of multilingual transformers for cross-lingual medical text classification in a zero-shot setting.**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
