{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load and preprocess your English-labeled data\n",
    "df = pd.read_csv(\"1900rows_data.csv\")[[\"text\", \"LABEL\"]]\n",
    "df = df.rename(columns={\"text\": \"phrase\", \"LABEL\": \"label\"})\n",
    "\n",
    "# Encode labels\n",
    "unique_labels = df[\"label\"].unique().tolist()\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "df[\"label_id\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_ds = Dataset.from_pandas(train_df[[\"phrase\", \"label_id\"]])\n",
    "test_ds = Dataset.from_pandas(test_df[[\"phrase\", \"label_id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b64a98819284806971e535067a0e293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1524 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552630704dc541128f812d9789e431d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/382 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"phrase\"], padding=\"max_length\", truncation=True, max_length=128\n",
    "    )\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(tokenize_function, batched=True)\n",
    "test_ds = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "train_ds = train_ds.rename_column(\"label_id\", \"labels\")\n",
    "test_ds = test_ds.rename_column(\"label_id\", \"labels\")\n",
    "train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/gj/nvvqk8_x3l56pvm3zf2l8g_r0000gn/T/ipykernel_51847/122630788.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\", num_labels=len(label2id)\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./multilingual_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa49508204f4c6cb2c82dc10bf8c66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/573 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1397, 'grad_norm': 12.840332984924316, 'learning_rate': 4.56369982547993e-05, 'epoch': 0.26}\n",
      "{'loss': 1.7763, 'grad_norm': 15.957270622253418, 'learning_rate': 4.12739965095986e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2525, 'grad_norm': 7.2755842208862305, 'learning_rate': 3.691099476439791e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e09cce88bd407182895e5dd709fed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9233168363571167, 'eval_accuracy': 0.7146596858638743, 'eval_f1': 0.6857415134116677, 'eval_runtime': 4.1268, 'eval_samples_per_second': 92.565, 'eval_steps_per_second': 11.631, 'epoch': 1.0}\n",
      "{'loss': 0.9348, 'grad_norm': 11.172807693481445, 'learning_rate': 3.254799301919721e-05, 'epoch': 1.05}\n",
      "{'loss': 0.6055, 'grad_norm': 2.9885454177856445, 'learning_rate': 2.8184991273996508e-05, 'epoch': 1.31}\n",
      "{'loss': 0.5925, 'grad_norm': 6.894710540771484, 'learning_rate': 2.382198952879581e-05, 'epoch': 1.57}\n",
      "{'loss': 0.5689, 'grad_norm': 22.988672256469727, 'learning_rate': 1.9458987783595115e-05, 'epoch': 1.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c9695d050041678e0b6121437184c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5195261240005493, 'eval_accuracy': 0.8481675392670157, 'eval_f1': 0.8429289395713538, 'eval_runtime': 3.6841, 'eval_samples_per_second': 103.689, 'eval_steps_per_second': 13.029, 'epoch': 2.0}\n",
      "{'loss': 0.4215, 'grad_norm': 21.366735458374023, 'learning_rate': 1.5095986038394417e-05, 'epoch': 2.09}\n",
      "{'loss': 0.2199, 'grad_norm': 11.411602020263672, 'learning_rate': 1.0732984293193717e-05, 'epoch': 2.36}\n",
      "{'loss': 0.2294, 'grad_norm': 1.2310848236083984, 'learning_rate': 6.369982547993019e-06, 'epoch': 2.62}\n",
      "{'loss': 0.2625, 'grad_norm': 19.81229591369629, 'learning_rate': 2.006980802792321e-06, 'epoch': 2.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a5e6edb4794cc78d6932b2b294aaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.40276291966438293, 'eval_accuracy': 0.8717277486910995, 'eval_f1': 0.8694356763187971, 'eval_runtime': 3.7316, 'eval_samples_per_second': 102.368, 'eval_steps_per_second': 12.863, 'epoch': 3.0}\n",
      "{'train_runtime': 228.1192, 'train_samples_per_second': 20.042, 'train_steps_per_second': 2.512, 'train_loss': 0.7933010181207307, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"multilingual_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Zero-Shot Inference on Chinese Text with Your Multilingual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_path = \"multilingual_output\"  # your saved model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "import pandas as pd\n",
    "\n",
    "# Load and sample your English data\n",
    "df = pd.read_csv(\"1900rows_data.csv\")[[\"text\", \"LABEL\"]].rename(\n",
    "    columns={\"text\": \"phrase\", \"LABEL\": \"label\"}\n",
    ")\n",
    "sample_df = df.sample(10, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Translate English to Chinese\n",
    "sample_df[\"translated_phrase\"] = sample_df[\"phrase\"].apply(\n",
    "    lambda x: GoogleTranslator(source=\"en\", target=\"zh-CN\").translate(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and predict\n",
    "inputs = tokenizer(\n",
    "    sample_df[\"translated_phrase\"].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>translated_phrase</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've been feeling really weak in my muscles an...</td>\n",
       "      <td>æˆ‘çš„è‚Œè‚‰æ„Ÿè§‰çœŸçš„å¾ˆè™šå¼±ï¼Œè„–å­çœŸçš„å¾ˆåƒµç¡¬ã€‚æˆ‘çš„å…³èŠ‚ä¸€ç›´åœ¨è‚¿èƒ€ï¼Œæˆ‘å¾ˆéš¾å››å¤„èµ°åŠ¨è€Œä¸ä¼šæ„Ÿåˆ°åƒµç¡¬ã€‚æ­¥...</td>\n",
       "      <td>Chronic Conditions</td>\n",
       "      <td>Chronic Conditions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when i extend my leg there is pain in knee joint</td>\n",
       "      <td>å½“æˆ‘ä¼¸å‡ºè…¿æ—¶ï¼Œè†å…³èŠ‚ä¼šç–¼ç—›</td>\n",
       "      <td>Pain-related Conditions</td>\n",
       "      <td>Pain-related Conditions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My bloody stools have caused me to lose a lot ...</td>\n",
       "      <td>æˆ‘çš„è¡€è…¥å‡³å­ä½¿æˆ‘å¤±å»äº†å¾ˆå¤šä¸œè¥¿ï¼ŒåŒ…æ‹¬é“å’Œè“è‰²ã€‚ç»“æœï¼Œæˆ‘ç°åœ¨æ‚£æœ‰è´«è¡€ï¼Œé€šå¸¸æˆ‘ä¼šæ„Ÿåˆ°å¾ˆè™šå¼±ã€‚</td>\n",
       "      <td>Gastrointestinal Conditions</td>\n",
       "      <td>Infections</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My skin is red and scratchy. These can occasio...</td>\n",
       "      <td>æˆ‘çš„çš®è‚¤æ˜¯çº¢è‰²çš„ã€‚è¿™äº›å¶å°”ä¼šå‰¥è½ã€‚æˆ‘çš„è„¸é¢Šå’Œå˜´å”‡è‚¿èƒ€ï¼Œè¿™çœŸçš„å¾ˆçƒ¦äººã€‚æˆ‘å¶å°”ä¼šå¤´ç—›å’Œæµé¼»æ¶•ï¼Œå› ...</td>\n",
       "      <td>Allergic/Immunologic Reactions</td>\n",
       "      <td>Allergic/Immunologic Reactions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have a cut on my foot that became infected f...</td>\n",
       "      <td>æˆ‘çš„è„šå‰²ä¼¤äº†ï¼Œç”±äºåœ¨å¥èº«æˆ¿ä½¿ç”¨æ·‹æµ´è€Œè¢«æ„ŸæŸ“ã€‚</td>\n",
       "      <td>Infections</td>\n",
       "      <td>Infections</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I have been experiencing symptoms such as a he...</td>\n",
       "      <td>æˆ‘ä¸€ç›´åœ¨é‡åˆ°ç—‡çŠ¶ï¼Œä¾‹å¦‚å¤´ç—›ï¼Œèƒ¸ç—›ï¼Œå¤´æ™•ï¼Œå¹³è¡¡ä¸§å¤±å’Œå›°éš¾ã€‚</td>\n",
       "      <td>Chronic Conditions</td>\n",
       "      <td>Chronic Conditions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>My knee hurts when I walk</td>\n",
       "      <td>æˆ‘èµ°è·¯æ—¶è†ç›–ç–¼</td>\n",
       "      <td>Pain-related Conditions</td>\n",
       "      <td>Pain-related Conditions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>It feels like I can't take a deep breath</td>\n",
       "      <td>æ„Ÿè§‰æˆ‘ä¸èƒ½æ·±å¸ä¸€å£æ°”</td>\n",
       "      <td>Respiratory &amp; Sensory Issues</td>\n",
       "      <td>Respiratory &amp; Sensory Issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I have a cut that is red and swollen.</td>\n",
       "      <td>æˆ‘æœ‰ä¸€ä¸ªçº¢è‰²å’Œè‚¿èƒ€çš„åˆ‡å£ã€‚</td>\n",
       "      <td>Infections</td>\n",
       "      <td>Dermatological &amp; Skin Conditions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I have a high temperature, vomiting, chills, a...</td>\n",
       "      <td>æˆ‘æœ‰é«˜æ¸©ï¼Œå‘•åï¼Œå‘å†·å’Œä¸¥é‡çš„ç˜™ç—’ã€‚æ­¤å¤–ï¼Œæˆ‘ä¸€ç›´åœ¨è¯´è¯å¾ˆå¤šï¼Œå¤´ç—›ã€‚æˆ‘ä¹Ÿå› æ¶å¿ƒå’Œè‚Œè‚‰ç–¼ç—›è€Œå›°æ‰°ã€‚</td>\n",
       "      <td>Infections</td>\n",
       "      <td>Allergic/Immunologic Reactions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              phrase  \\\n",
       "0  I've been feeling really weak in my muscles an...   \n",
       "1   when i extend my leg there is pain in knee joint   \n",
       "2  My bloody stools have caused me to lose a lot ...   \n",
       "3  My skin is red and scratchy. These can occasio...   \n",
       "4  I have a cut on my foot that became infected f...   \n",
       "5  I have been experiencing symptoms such as a he...   \n",
       "6                          My knee hurts when I walk   \n",
       "7           It feels like I can't take a deep breath   \n",
       "8              I have a cut that is red and swollen.   \n",
       "9  I have a high temperature, vomiting, chills, a...   \n",
       "\n",
       "                                   translated_phrase  \\\n",
       "0  æˆ‘çš„è‚Œè‚‰æ„Ÿè§‰çœŸçš„å¾ˆè™šå¼±ï¼Œè„–å­çœŸçš„å¾ˆåƒµç¡¬ã€‚æˆ‘çš„å…³èŠ‚ä¸€ç›´åœ¨è‚¿èƒ€ï¼Œæˆ‘å¾ˆéš¾å››å¤„èµ°åŠ¨è€Œä¸ä¼šæ„Ÿåˆ°åƒµç¡¬ã€‚æ­¥...   \n",
       "1                                      å½“æˆ‘ä¼¸å‡ºè…¿æ—¶ï¼Œè†å…³èŠ‚ä¼šç–¼ç—›   \n",
       "2       æˆ‘çš„è¡€è…¥å‡³å­ä½¿æˆ‘å¤±å»äº†å¾ˆå¤šä¸œè¥¿ï¼ŒåŒ…æ‹¬é“å’Œè“è‰²ã€‚ç»“æœï¼Œæˆ‘ç°åœ¨æ‚£æœ‰è´«è¡€ï¼Œé€šå¸¸æˆ‘ä¼šæ„Ÿåˆ°å¾ˆè™šå¼±ã€‚   \n",
       "3  æˆ‘çš„çš®è‚¤æ˜¯çº¢è‰²çš„ã€‚è¿™äº›å¶å°”ä¼šå‰¥è½ã€‚æˆ‘çš„è„¸é¢Šå’Œå˜´å”‡è‚¿èƒ€ï¼Œè¿™çœŸçš„å¾ˆçƒ¦äººã€‚æˆ‘å¶å°”ä¼šå¤´ç—›å’Œæµé¼»æ¶•ï¼Œå› ...   \n",
       "4                             æˆ‘çš„è„šå‰²ä¼¤äº†ï¼Œç”±äºåœ¨å¥èº«æˆ¿ä½¿ç”¨æ·‹æµ´è€Œè¢«æ„ŸæŸ“ã€‚   \n",
       "5                       æˆ‘ä¸€ç›´åœ¨é‡åˆ°ç—‡çŠ¶ï¼Œä¾‹å¦‚å¤´ç—›ï¼Œèƒ¸ç—›ï¼Œå¤´æ™•ï¼Œå¹³è¡¡ä¸§å¤±å’Œå›°éš¾ã€‚   \n",
       "6                                            æˆ‘èµ°è·¯æ—¶è†ç›–ç–¼   \n",
       "7                                         æ„Ÿè§‰æˆ‘ä¸èƒ½æ·±å¸ä¸€å£æ°”   \n",
       "8                                      æˆ‘æœ‰ä¸€ä¸ªçº¢è‰²å’Œè‚¿èƒ€çš„åˆ‡å£ã€‚   \n",
       "9     æˆ‘æœ‰é«˜æ¸©ï¼Œå‘•åï¼Œå‘å†·å’Œä¸¥é‡çš„ç˜™ç—’ã€‚æ­¤å¤–ï¼Œæˆ‘ä¸€ç›´åœ¨è¯´è¯å¾ˆå¤šï¼Œå¤´ç—›ã€‚æˆ‘ä¹Ÿå› æ¶å¿ƒå’Œè‚Œè‚‰ç–¼ç—›è€Œå›°æ‰°ã€‚   \n",
       "\n",
       "                            label                   predicted_label  \n",
       "0              Chronic Conditions                Chronic Conditions  \n",
       "1         Pain-related Conditions           Pain-related Conditions  \n",
       "2     Gastrointestinal Conditions                        Infections  \n",
       "3  Allergic/Immunologic Reactions    Allergic/Immunologic Reactions  \n",
       "4                      Infections                        Infections  \n",
       "5              Chronic Conditions                Chronic Conditions  \n",
       "6         Pain-related Conditions           Pain-related Conditions  \n",
       "7    Respiratory & Sensory Issues      Respiratory & Sensory Issues  \n",
       "8                      Infections  Dermatological & Skin Conditions  \n",
       "9                      Infections    Allergic/Immunologic Reactions  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label mapping (rebuild from training)\n",
    "unique_labels = df[\"label\"].unique().tolist()\n",
    "id2label = {i: label for i, label in enumerate(unique_labels)}\n",
    "\n",
    "# Add predictions to DataFrame\n",
    "sample_df[\"predicted_label\"] = [id2label[p.item()] for p in predictions]\n",
    "\n",
    "# Show results\n",
    "sample_df[[\"phrase\", \"translated_phrase\", \"label\", \"predicted_label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We fine-tuned bert-base-multilingual-cased on 1,900 English-labeled medical symptom examples. We then evaluated the model on Chinese-translated versions of those examples, without any fine-tuning on Chinese data. The model correctly classified most inputs, achieving strong semantic generalization across languages. These results demonstrate the power of multilingual transformers for cross-lingual medical text classification in a zero-shot setting.**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
